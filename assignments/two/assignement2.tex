%%%%%latex preamble%%%%%

\documentclass[titlepage]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage[sc]{mathpazo}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=1cm,rmargin=1cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{pdfstartview={XYZ null null 1}}
\usepackage{float}
\usepackage{bm}
\usepackage{tikz}
 %changes default sectioning commands
%\usepackage{breakurl}
\renewcommand{\thesubsection}{(\alph{subsection})}
\renewcommand{\thesubsubsection}{\roman{subsection}.}
\usepackage{lastpage}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{}
\chead{\leftmark}
\rhead{}
\lfoot{Aaron Gonzales; Data Mining}
\cfoot{Homework 13}
\rfoot{Page \thepage\ of \pageref{LastPage}}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\title{Data Mining: Assignment Two}
\author{Aaron Gonzales}
\maketitle

%%%%%%%%knitr option%%%%%%%




\section{Neural Network}

Reading the data, optimizing for speed:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{tab5rows} \hlkwb{<-} \hlkwd{read.table}\hlstd{(}\hlstr{"~/Dropbox/cs/datamining/data-mining-is-fun/assignments/two/data.txt"}\hlstd{,} \hlkwc{header} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlkwc{nrows} \hlstd{=} \hlnum{5}\hlstd{)}
\hlstd{classes} \hlkwb{<-} \hlkwd{sapply}\hlstd{(tab5rows, class)}
\hlstd{data} \hlkwb{<-} \hlkwd{read.table}\hlstd{(}\hlstr{"~/Dropbox/cs/datamining/data-mining-is-fun/assignments/two/data.txt"}\hlstd{,} \hlkwc{header} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlkwc{colClasses} \hlstd{= classes,} \hlkwc{nrows} \hlstd{=} \hlnum{11000}\hlstd{,} \hlkwc{comment.char}\hlstd{=}\hlstr{""}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\subsection {}

Using the nnet package and a small function to tabulate our predictions.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(nnet)}
\hlkwd{library}\hlstd{(caret)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: lattice}}\begin{alltt}
\hlkwd{library}\hlstd{(pROC)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Type 'citation("{}pROC"{})' for a citation.\\\#\# \\\#\# Attaching package: 'pROC'\\\#\# \\\#\# The following objects are masked from 'package:stats':\\\#\# \\\#\#\ \ \ \  cov, smooth, var}}\begin{alltt}
\hlkwd{nrow}\hlstd{(data)}
\end{alltt}
\begin{verbatim}
## [1] 10000
\end{verbatim}
\begin{alltt}
\hlcom{#Define class 'labels' for our set.}
\hlstd{targets} \hlkwb{<-} \hlkwd{class.ind}\hlstd{(}\hlkwd{c}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlstr{'0'}\hlstd{,} \hlnum{5000}\hlstd{),} \hlkwd{rep}\hlstd{(}\hlstr{'1'}\hlstd{,}\hlnum{5000}\hlstd{)))}
\hlstd{labels} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{5000}\hlstd{),} \hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{5000}\hlstd{))}
\hlstd{data}\hlopt{$}\hlstd{labels} \hlkwb{<-} \hlstd{labels}
\hlstd{data}\hlopt{$}\hlstd{labels} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(data}\hlopt{$}\hlstd{labels)}

\hlcom{# small function to show predicted results}
\hlstd{test.cl} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{true}\hlstd{,} \hlkwc{pred}\hlstd{) \{}
    \hlstd{actual} \hlkwb{<-} \hlkwd{max.col}\hlstd{(true)} \hlopt{-} \hlnum{1}
    \hlstd{predicted} \hlkwb{<-} \hlkwd{max.col}\hlstd{(pred)} \hlopt{-} \hlnum{1}
    \hlkwd{table}\hlstd{(actual, predicted)}

\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# caret has many functions to help chop up data, here we partition the data}
\hlcom{# into a test and training set}
\hlkwd{library}\hlstd{(doMC)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: foreach\\\#\# Loading required package: iterators\\\#\# Loading required package: parallel}}\begin{alltt}
\hlcom{# sets multiple cores to help run the svm}
\hlkwd{registerDoMC}\hlstd{(}\hlnum{2}\hlstd{)}
\hlstd{data_small} \hlkwb{<-} \hlkwd{subset}\hlstd{(data,} \hlkwc{select}\hlstd{=}\hlkwd{c}\hlstd{(V1, V2, V3, V4, labels))}
\hlstd{inTrain} \hlkwb{<-} \hlkwd{createDataPartition}\hlstd{(}\hlkwc{y} \hlstd{= data_small}\hlopt{$}\hlstd{labels,} \hlkwc{p}\hlstd{=}\hlnum{.75}\hlstd{,} \hlkwc{list}\hlstd{=}\hlnum{FALSE}\hlstd{)}
\hlstd{training} \hlkwb{<-} \hlstd{data_small[inTrain,]}
\hlstd{testing} \hlkwb{<-} \hlstd{data_small[}\hlopt{-}\hlstd{inTrain,]}

\hlcom{# here is a control model that specifies to use repeated 10-fold cross validation}
\hlstd{ctrl} \hlkwb{<-} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{'repeatedcv'}\hlstd{,}
                     \hlkwc{number} \hlstd{=} \hlnum{10}
                     \hlstd{)}

\hlcom{# we train the model on the training data and with the labeling data as a guide}
\hlstd{fitsmall} \hlkwb{<-} \hlkwd{train}\hlstd{( training, training}\hlopt{$}\hlstd{labels,}
              \hlkwc{method} \hlstd{=} \hlstr{"nnet"}\hlstd{,} \hlcom{# uses the 'nnet' package as a backend}
              \hlkwc{algorithm} \hlstd{=} \hlstr{'backprop'}\hlstd{,}
              \hlkwc{learningrate} \hlstd{=} \hlnum{0.1}\hlstd{,}
              \hlkwc{trControl} \hlstd{= ctrl,}
              \hlkwc{linout} \hlstd{=} \hlnum{FALSE}\hlstd{,}
              \hlkwc{MaxNWts} \hlstd{=} \hlnum{15000}\hlstd{)}
\hlcom{## many pages of printing supressed}
\end{alltt}
\end{kframe}
\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#shows the confusion matrix}
\hlkwd{confusionMatrix}\hlstd{(fitsmall)}
\end{alltt}
\begin{verbatim}
## Cross-Validated (10 fold, repeated 1 times) Confusion Matrix 
## 
## (entries are percentages of table totals)
##  
##           Reference
## Prediction  0  1
##          0 50  0
##          1  0 50
\end{verbatim}
\begin{alltt}
\hlcom{#prediction}
\hlstd{smallpre} \hlkwb{<-} \hlkwd{predict}\hlstd{(fitsmall,} \hlkwc{newdata} \hlstd{= testing)}
\hlkwd{summary}\hlstd{(smallpre)}
\end{alltt}
\begin{verbatim}
##    0    1 
## 1250 1250
\end{verbatim}
\begin{alltt}
\hlstd{a} \hlkwb{<-} \hlkwd{test.cl}\hlstd{(testing}\hlopt{$}\hlstd{labels,} \hlkwd{predict}\hlstd{(fitsmall, testing))}
\hlkwd{print}\hlstd{(a)}
\end{alltt}
\begin{verbatim}
##       predicted
## actual    0
##      0 2500
\end{verbatim}
\end{kframe}
\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# caret has many functions to help chop up data, here we partition the data}
\hlcom{# into a test and training set}
\hlstd{inTrain} \hlkwb{<-} \hlkwd{createDataPartition}\hlstd{(}\hlkwc{y} \hlstd{= data}\hlopt{$}\hlstd{labels,} \hlkwc{p}\hlstd{=}\hlnum{.75}\hlstd{,} \hlkwc{list}\hlstd{=}\hlnum{FALSE}\hlstd{)}
\hlstd{training} \hlkwb{<-} \hlstd{data[inTrain,]}
\hlstd{testing} \hlkwb{<-} \hlstd{data[}\hlopt{-}\hlstd{inTrain,]}

\hlcom{# here is a control model that specifies to use repeated 10-fold cross validation}
\hlstd{ctrl} \hlkwb{<-} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{'cv'}\hlstd{,}
                     \hlkwc{number} \hlstd{=} \hlnum{10}
                     \hlstd{)}

\hlcom{# we train the model on the training data and with the labeling data as a guide}
\hlstd{fitnn} \hlkwb{<-} \hlkwd{train}\hlstd{( training, training}\hlopt{$}\hlstd{labels,}
              \hlkwc{method} \hlstd{=} \hlstr{"nnet"}\hlstd{,} \hlcom{# uses the 'nnet' package as a backend}
              \hlkwc{algorithm} \hlstd{=} \hlstr{'backprop'}\hlstd{,}
              \hlkwc{learningrate} \hlstd{=} \hlnum{0.2}\hlstd{,}
              \hlkwc{trControl} \hlstd{= ctrl,}
              \hlkwc{linout} \hlstd{=} \hlnum{FALSE}\hlstd{,}
              \hlkwc{MaxNWts} \hlstd{=} \hlnum{15000}\hlstd{,}
              \hlkwc{maxit} \hlstd{=} \hlnum{100}\hlstd{,}
              \hlkwc{hidden} \hlstd{=} \hlnum{10}\hlstd{)}
\hlcom{## many pages of printing supressed}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#shows the confusion matrix}
\hlkwd{confusionMatrix}\hlstd{(fitnn)}
\end{alltt}
\begin{verbatim}
## Cross-Validated (10 fold) Confusion Matrix 
## 
## (entries are percentages of table totals)
##  
##           Reference
## Prediction    0    1
##          0 45.2  2.6
##          1  4.8 47.4
\end{verbatim}
\begin{alltt}
\hlcom{#prediction}
\hlstd{pre} \hlkwb{<-} \hlkwd{predict}\hlstd{(fitnn,} \hlkwc{newdata} \hlstd{= testing)}
\hlkwd{summary}\hlstd{(pre)}
\end{alltt}
\begin{verbatim}
##    0    1 
## 1226 1274
\end{verbatim}
\begin{alltt}
\hlstd{a} \hlkwb{<-} \hlkwd{test.cl}\hlstd{(testing}\hlopt{$}\hlstd{labels,} \hlkwd{predict}\hlstd{(fitnn, testing))}
\hlkwd{print}\hlstd{(a)}
\end{alltt}
\begin{verbatim}
##       predicted
## actual    0
##      0 2500
\end{verbatim}
\end{kframe}
\end{knitrout}
Neural network...


We use the caret (classification and regression tool) package with doMC(Parallelization for R) to train an svm (libsvm package) with a radial basis function.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(doMC)}
\hlcom{# sets multiple cores to help run the svm}
\hlkwd{registerDoMC}\hlstd{(}\hlnum{2}\hlstd{)}
\hlcom{# subsetting into train and test sets.}
\hlstd{index} \hlkwb{<-} \hlnum{1}\hlopt{:}\hlkwd{nrow}\hlstd{(data)}
\hlstd{testindex} \hlkwb{<-} \hlkwd{sample}\hlstd{(index,} \hlkwd{trunc}\hlstd{(}\hlkwd{length}\hlstd{(index)}\hlopt{*}\hlnum{30}\hlopt{/}\hlnum{100}\hlstd{))}
\hlstd{testset} \hlkwb{<-} \hlstd{data[testindex,]}
\hlstd{trainset} \hlkwb{<-} \hlstd{data[}\hlopt{-}\hlstd{testindex,]}

\hlcom{# training the model with a radial basis function.}
\hlcom{# uses the lables ~ . to say 'the class labels are defined in the varible}
\hlcom{# 'labels'. 10-fold cross validation is performed in the model and reported later and}
\hlcom{# doesn't need to be done by hand.}
\hlkwd{system.time}\hlstd{(}
  \hlstd{fitsvm} \hlkwb{<-} \hlkwd{train}\hlstd{(labels} \hlopt{~} \hlstd{.,}
                 \hlkwc{data} \hlstd{= trainset,}
                 \hlkwc{method}\hlstd{=}\hlstr{"svmRadial"}\hlstd{,}
                 \hlkwc{trControl}\hlstd{=}\hlkwd{trainControl}\hlstd{(}\hlkwc{method}\hlstd{=}\hlstr{'cv'}\hlstd{,} \hlkwc{number} \hlstd{=} \hlnum{10}\hlstd{)}
                 \hlstd{)}
\hlstd{)}
\end{alltt}
\begin{verbatim}
##    user  system elapsed 
## 789.785   7.378 915.254
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{print}\hlstd{(fitsvm)}
\end{alltt}
\begin{verbatim}
## Support Vector Machines with Radial Basis Function Kernel 
## 
## 7000 samples
## 1000 predictors
##    2 classes: '0', '1' 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## 
## Summary of sample sizes: 6300, 6300, 6301, 6300, 6300, 6300, ... 
## 
## Resampling results across tuning parameters:
## 
##   C    Accuracy  Kappa  Accuracy SD  Kappa SD
##   0.2  1         1      1e-03        2e-03   
##   0.5  1         1      0e+00        0e+00   
##   1.0  1         1      5e-04        9e-04   
## 
## Tuning parameter 'sigma' was held constant at a value of 0.0005525
## Accuracy was used to select the optimal model using  the largest value.
## The final values used for the model were sigma = 0.0005525 and C = 0.5.
\end{verbatim}
\begin{alltt}
\hlcom{# prediction.}
\hlstd{prediction} \hlkwb{<-} \hlkwd{predict}\hlstd{(fitsvm, testset[,}\hlopt{-}\hlnum{1001}\hlstd{])}
\hlstd{tab} \hlkwb{<-} \hlkwd{table}\hlstd{(}\hlkwc{pred} \hlstd{= prediction,} \hlkwc{true} \hlstd{= testset}\hlopt{$}\hlstd{labels)}
\hlkwd{plot}\hlstd{(fitsvm)}
\hlkwd{confusionMatrix}\hlstd{(fitsvm)}
\end{alltt}
\begin{verbatim}
## Cross-Validated (10 fold) Confusion Matrix 
## 
## (entries are percentages of table totals)
##  
##           Reference
## Prediction    0    1
##          0 49.6  0.0
##          1  0.0 50.4
\end{verbatim}
\begin{alltt}
\hlkwd{print}\hlstd{(tab)}
\end{alltt}
\begin{verbatim}
##     true
## pred    0    1
##    0 1529    0
##    1    0 1471
\end{verbatim}
\begin{alltt}
\hlcom{# model accuracy}
\hlkwd{print}\hlstd{(}\hlnum{1}\hlopt{-}\hlstd{(tab[}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{]}\hlopt{+}\hlstd{tab[}\hlnum{2}\hlstd{,}\hlnum{1}\hlstd{])} \hlopt{/} \hlstd{(tab[}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{]}\hlopt{+}\hlstd{tab[}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{]))}
\end{alltt}
\begin{verbatim}
## [1] 1
\end{verbatim}
\end{kframe}

{\centering \includegraphics[width=.45\linewidth]{figure/minimal-svm1_print} 

}



\end{knitrout}


\section{Scrambled data}
Here we assign the classes according to the assignment rubric, divided in chunks of 500.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{data_copy} \hlkwb{<-} \hlstd{data}

\hlstd{labels} \hlkwb{<-} \hlkwd{c}\hlstd{(}
  \hlkwd{rep}\hlstd{(}\hlstr{'0'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'1'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'0'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'1'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'0'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'1'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'0'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'1'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'0'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'1'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'0'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'1'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'0'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'1'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'0'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'1'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'0'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'1'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'0'}\hlstd{,}\hlnum{500}\hlstd{),}
        \hlkwd{rep}\hlstd{(}\hlstr{'1'}\hlstd{,}\hlnum{500}\hlstd{)}
\hlstd{)}

\hlstd{data_copy}\hlopt{$}\hlstd{labels} \hlkwb{<-} \hlkwd{as.factor}\hlstd{(labels)}
\hlkwd{is.factor}\hlstd{(data_copy}\hlopt{$}\hlstd{labels)}
\end{alltt}
\begin{verbatim}
## [1] TRUE
\end{verbatim}
\end{kframe}
\end{knitrout}

\subsection{Neural Network, again}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{inTrain} \hlkwb{<-} \hlkwd{createDataPartition}\hlstd{(}\hlkwc{y} \hlstd{= data_copy}\hlopt{$}\hlstd{labels,} \hlkwc{p}\hlstd{=}\hlnum{.71}\hlstd{,} \hlkwc{list}\hlstd{=}\hlnum{FALSE}\hlstd{)}
\hlstd{training} \hlkwb{<-} \hlstd{data_copy[inTrain,]}
\hlstd{testing} \hlkwb{<-} \hlstd{data_copy[}\hlopt{-}\hlstd{inTrain,]}

\hlstd{ctrl} \hlkwb{<-} \hlkwd{trainControl}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{'repeatedcv'}\hlstd{,}
                     \hlkwc{number} \hlstd{=} \hlnum{10}
                     \hlstd{)}

\hlkwd{is.factor}\hlstd{(training}\hlopt{$}\hlstd{labels)}
\hlstd{fitnn2} \hlkwb{<-} \hlkwd{train}\hlstd{( training, training}\hlopt{$}\hlstd{labels,}
              \hlkwc{method} \hlstd{=} \hlstr{"nnet"}\hlstd{,}
              \hlkwc{algorithm} \hlstd{=} \hlstr{'backprop'}\hlstd{,}
              \hlkwc{learningrate} \hlstd{=} \hlnum{0.1}\hlstd{,}
              \hlkwc{trControl} \hlstd{= ctrl,}
              \hlkwc{linout} \hlstd{=} \hlnum{FALSE}\hlstd{,}
              \hlcom{## this was the important thing to pass to nnet}
              \hlkwc{MaxNWts} \hlstd{=} \hlnum{15000}\hlstd{)}

\hlcom{## many pages of printing output is supressed here}
\end{alltt}
\end{kframe}
\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{confusionMatrix}\hlstd{(fitnn2)}
\end{alltt}
\begin{verbatim}
## Cross-Validated (10 fold, repeated 1 times) Confusion Matrix 
## 
## (entries are percentages of table totals)
##  
##           Reference
## Prediction    0    1
##          0 46.2  2.8
##          1  3.8 47.2
\end{verbatim}
\begin{alltt}
\hlcom{#prediction}
\hlstd{pre} \hlkwb{<-} \hlkwd{predict}\hlstd{(fitnn2,} \hlkwc{newdata} \hlstd{= testing)}
\hlkwd{summary}\hlstd{(pre)}
\end{alltt}
\begin{verbatim}
##    0    1 
## 1445 1455
\end{verbatim}
\begin{alltt}
\hlstd{a} \hlkwb{<-} \hlkwd{test.cl}\hlstd{(testing}\hlopt{$}\hlstd{labels,} \hlkwd{predict}\hlstd{(fitnn2, testing))}
\hlkwd{print}\hlstd{(a)}
\end{alltt}
\begin{verbatim}
##       predicted
## actual    0
##      0 2900
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsection{SVM, again}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# subsetting into train and test sets.}
\hlstd{index} \hlkwb{<-} \hlnum{1}\hlopt{:}\hlkwd{nrow}\hlstd{(data_copy)}
\hlstd{testindex} \hlkwb{<-} \hlkwd{sample}\hlstd{(index,} \hlkwd{trunc}\hlstd{(}\hlkwd{length}\hlstd{(index)}\hlopt{*}\hlnum{30}\hlopt{/}\hlnum{100}\hlstd{))}
\hlstd{testset} \hlkwb{<-} \hlstd{data_copy[testindex,]}
\hlstd{trainset} \hlkwb{<-} \hlstd{data_copy[}\hlopt{-}\hlstd{testindex,]}

\hlcom{# training the model with a radial basis function.}
\hlcom{# uses the lables ~ . to say 'the class labels are defined in the varible}
\hlcom{# 'labels'. 10-fold cross validation is performed in the model and reported later and}
\hlcom{# doesn't need to be done by hand.}
\hlkwd{system.time}\hlstd{(}
  \hlstd{model} \hlkwb{<-} \hlkwd{train}\hlstd{(labels} \hlopt{~} \hlstd{.,}
                 \hlkwc{data} \hlstd{= trainset,}
                 \hlkwc{method}\hlstd{=}\hlstr{"svmRadial"}\hlstd{,}
                 \hlkwc{trControl}\hlstd{=}\hlkwd{trainControl}\hlstd{(}\hlkwc{method}\hlstd{=}\hlstr{'cv'}\hlstd{,} \hlkwc{number} \hlstd{=} \hlnum{10}\hlstd{)}
                 \hlstd{)}
\hlstd{)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: kernlab}}\begin{verbatim}
##    user  system elapsed 
## 4286.61   27.85 4347.85
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{print}\hlstd{(model)}
\end{alltt}
\begin{verbatim}
## Support Vector Machines with Radial Basis Function Kernel 
## 
## 7000 samples
## 1000 predictors
##    2 classes: '0', '1' 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## 
## Summary of sample sizes: 6299, 6300, 6300, 6300, 6300, 6300, ... 
## 
## Resampling results across tuning parameters:
## 
##   C    Accuracy  Kappa  Accuracy SD  Kappa SD
##   0.2  0.5       -0.01  0.007        0.01    
##   0.5  0.5       -0.02  0.017        0.03    
##   1.0  0.5       -0.02  0.015        0.03    
## 
## Tuning parameter 'sigma' was held constant at a value of 0.0005511
## Accuracy was used to select the optimal model using  the largest value.
## The final values used for the model were sigma = 0.0005511 and C = 0.25.
\end{verbatim}
\begin{alltt}
\hlcom{# prediction.}
\hlstd{prediction} \hlkwb{<-} \hlkwd{predict}\hlstd{(model, testset[,}\hlopt{-}\hlnum{1001}\hlstd{])}
\hlstd{tab} \hlkwb{<-} \hlkwd{table}\hlstd{(}\hlkwc{pred} \hlstd{= prediction,} \hlkwc{true} \hlstd{= testset}\hlopt{$}\hlstd{labels)}
\hlkwd{plot}\hlstd{(model)}
\hlkwd{confusionMatrix}\hlstd{(model)}
\end{alltt}
\begin{verbatim}
## Cross-Validated (10 fold) Confusion Matrix 
## 
## (entries are percentages of table totals)
##  
##           Reference
## Prediction    0    1
##          0  4.1  4.9
##          1 45.3 45.6
\end{verbatim}
\begin{alltt}
\hlkwd{print}\hlstd{(tab)}
\end{alltt}
\begin{verbatim}
##     true
## pred    0    1
##    0  153  154
##    1 1386 1307
\end{verbatim}
\begin{alltt}
\hlcom{# model accuracy}
\hlkwd{print}\hlstd{(}\hlnum{1}\hlopt{-}\hlstd{(tab[}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{]}\hlopt{+}\hlstd{tab[}\hlnum{2}\hlstd{,}\hlnum{1}\hlstd{])} \hlopt{/} \hlstd{(tab[}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{]}\hlopt{+}\hlstd{tab[}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{]))}
\end{alltt}
\begin{verbatim}
## [1] -0.05479
\end{verbatim}
\end{kframe}

{\centering \includegraphics[width=.45\linewidth]{figure/minimal-svm2print} 

}



\end{knitrout}


\end{document}
